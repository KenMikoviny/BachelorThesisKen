{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import torch_geometric.data as geom_data\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Entities\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from torch_geometric.nn import RGCNConv, FastRGCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "dataset = Entities(cwd, \"MUTAG\")\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "available dataset info includes:  'name',\n",
    " 'num_classes',\n",
    " 'num_edge_features',\n",
    " 'num_features',\n",
    " 'num_node_features',\n",
    " 'num_relations',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(edge_index=[2, 148454], edge_type=[148454], test_idx=[68], test_y=[68], train_idx=[272], train_y=[272])\n",
      "Length: 1\n",
      "Dataset:  MUTAGEntities()\n",
      "46 2\n"
     ]
    }
   ],
   "source": [
    "# Data exploration cell\n",
    "\n",
    "print(\"Data object:\", dataset.data)\n",
    "print(\"Length:\", len(dataset))\n",
    "print(\"Dataset: \", dataset)\n",
    "#print(\"Average label: %4.2f\" % (dataset.data.y.float().mean().item()))\n",
    "print(dataset.num_relations, dataset.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "## To get uniform embedding weights of custom range\n",
    "def uniform_embeddings(num_nodes, emb_dim, device=None):\n",
    "    uniform_distribution = Uniform(torch.tensor([-100.0]), torch.tensor([100.0]))\n",
    "\n",
    "    # Generate random center between -100 and 100\n",
    "    node_embeddings = uniform_distribution.sample((num_nodes, int(emb_dim))).squeeze(-1)\n",
    "    if device:\n",
    "        node_embeddings = node_embeddings.to(device)\n",
    "    node_embeddings.requires_grad = True\n",
    "\n",
    "    return node_embeddings\n",
    "\n",
    "## To get default embeddings,weights around 0 use:\n",
    "## Embedding (num_nodes, emb_dim)\n",
    "embedding = torch.nn.Embedding(10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch geometric automatically implements batching multiple graphs into 1 huge block diagonal adjacency matrix \n",
    "# + concatenates feature matrices etc.\n",
    "#graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset\n",
    "#graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BGS and AM graphs are too big to process them in a full-batch fashion.\n",
    "# Since our model does only make use of a rather small receptive field, we\n",
    "# filter the graph to only contain the nodes that are at most 2-hop neighbors\n",
    "# away from any training/test node.\n",
    "\n",
    "# k_hop_subgraph\n",
    "# Computes the k-hop subgraph of edge_index around node node_idx, returns:\n",
    "# (1) the nodes involved in the subgraph, \n",
    "# (2) the filtered edge_index connectivity, \n",
    "# (3) the mapping from node indices in node_idx to their new location, and \n",
    "# (4) the edge mask indicating which edges were preserved.\n",
    "\n",
    "node_idx = torch.cat([data.train_idx, data.test_idx], dim=0)\n",
    "node_idx, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "    node_idx, 2, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "data.num_nodes = node_idx.size(0)\n",
    "data.edge_index = edge_index\n",
    "data.edge_type = data.edge_type[edge_mask]\n",
    "data.train_idx = mapping[:data.train_idx.size(0)]\n",
    "data.test_idx = mapping[data.train_idx.size(0):]\n",
    "\n",
    "np.unique(data.edge_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 38 graphs stacked together for the test dataset. The batch indices, stored in batch, show that the first 12 nodes belong to the first graph, the next 22 to the second graph, and so on.\n",
    "\n",
    "These indices are important for performing the final prediction. To perform a prediction over a whole graph, we usually perform a pooling operation over all nodes after running the GNN model. In this case, we will use the average pooling. Hence, we need to know which nodes should be included in which average pool. Using this pooling, we can already create our graph network below. Specifically, we re-use our class GNNModel from before, and simply add an average pool and single linear layer for the graph prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "embedding_dim = 16\n",
    "num_bases = 30\n",
    "\n",
    "# Create initial embedding = 2D tensor (num_nodes,embedding_dim)\n",
    "untrained_embedding = uniform_embeddings(data.num_nodes, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 93.1385, -18.0757,  71.2376,  ..., -13.5010, -61.0333, -87.1491],\n",
       "        [-79.4696, -98.9854, -53.3307,  ..., -96.5766,  -0.3052, -50.2125],\n",
       "        [ 21.8054, -41.5483, -39.2340,  ...,  44.3851, -95.2284, -38.6289],\n",
       "        ...,\n",
       "        [ 74.9765,   3.0612,  86.9940,  ..., -36.3838,   7.5832,  13.8613],\n",
       "        [ 62.7756, -10.8657,  90.2944,  ...,  70.3103, -61.2729,  78.3486],\n",
       "        [-65.1617,  95.2100,  18.0446,  ..., -43.2312,  54.8700, -40.9425]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with 2 RGCNConv layers, input = node embeddings\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.node_embeddings = []\n",
    "        \n",
    "        self.conv1 = RGCNConv(in_channels=embedding_dim, out_channels=embedding_dim, num_relations=dataset.num_relations,\n",
    "                              num_bases=num_bases)\n",
    "        self.conv2 = RGCNConv(in_channels=embedding_dim, out_channels=embedding_dim, num_relations=dataset.num_relations,\n",
    "                              num_bases=num_bases)\n",
    "        \n",
    "\n",
    "    def forward(self, embedding, edge_index, edge_type):\n",
    "        x = F.relu(self.conv1(embedding, edge_index, edge_type))\n",
    "        x = self.conv2(x, edge_index, edge_type)   \n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        # Here we save node embeddings for all nodes = shape [23606,2]\n",
    "        self.node_embeddings = x\n",
    "        return x\n",

    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 266.2306, Train: 0.4632 Test: 0.4412\n",
      "Epoch: 02, Loss: 61.0120, Train: 0.6066 Test: 0.6618\n",
      "Epoch: 03, Loss: 74.8870, Train: 0.6066 Test: 0.6765\n",
      "Epoch: 04, Loss: 56.3706, Train: 0.5588 Test: 0.5000\n",
      "Epoch: 05, Loss: 31.7682, Train: 0.4228 Test: 0.3676\n",
      "Epoch: 06, Loss: 56.9123, Train: 0.6287 Test: 0.6029\n",
      "Epoch: 07, Loss: 18.7759, Train: 0.6250 Test: 0.6765\n",
      "Epoch: 08, Loss: 36.2677, Train: 0.6618 Test: 0.6765\n",
      "Epoch: 09, Loss: 28.0958, Train: 0.6471 Test: 0.5588\n",
      "Epoch: 10, Loss: 14.0498, Train: 0.5257 Test: 0.4412\n",
      "Epoch: 11, Loss: 27.3179, Train: 0.6618 Test: 0.6029\n",
      "Epoch: 12, Loss: 15.1853, Train: 0.7353 Test: 0.6618\n",
      "Epoch: 13, Loss: 14.9577, Train: 0.6949 Test: 0.6765\n",
      "Epoch: 14, Loss: 19.2661, Train: 0.7463 Test: 0.6618\n",
      "Epoch: 15, Loss: 12.2312, Train: 0.7169 Test: 0.6176\n",
      "Epoch: 16, Loss: 9.0807, Train: 0.6838 Test: 0.5882\n",
      "Epoch: 17, Loss: 13.2620, Train: 0.7610 Test: 0.6324\n",
      "Epoch: 18, Loss: 7.6053, Train: 0.7757 Test: 0.6618\n",
      "Epoch: 19, Loss: 7.5922, Train: 0.7279 Test: 0.6765\n",
      "Epoch: 20, Loss: 10.3734, Train: 0.7794 Test: 0.6324\n",
      "Epoch: 21, Loss: 6.7332, Train: 0.7904 Test: 0.6324\n",
      "Epoch: 22, Loss: 5.5572, Train: 0.7426 Test: 0.6176\n",
      "Epoch: 23, Loss: 7.8605, Train: 0.8125 Test: 0.6471\n",
      "Epoch: 24, Loss: 4.7524, Train: 0.7941 Test: 0.6765\n",
      "Epoch: 25, Loss: 4.5311, Train: 0.8125 Test: 0.7206\n",
      "Epoch: 26, Loss: 5.5436, Train: 0.8309 Test: 0.7059\n",
      "Epoch: 27, Loss: 3.1373, Train: 0.7794 Test: 0.6765\n",
      "Epoch: 28, Loss: 3.8946, Train: 0.7831 Test: 0.6765\n",
      "Epoch: 29, Loss: 3.5127, Train: 0.8603 Test: 0.7059\n",
      "Epoch: 30, Loss: 2.2391, Train: 0.8382 Test: 0.7353\n",
      "Epoch: 31, Loss: 2.9282, Train: 0.8750 Test: 0.7353\n",
      "Epoch: 32, Loss: 1.7698, Train: 0.8493 Test: 0.6765\n",
      "Epoch: 33, Loss: 1.6229, Train: 0.8346 Test: 0.6765\n",
      "Epoch: 34, Loss: 1.9185, Train: 0.9007 Test: 0.6912\n",
      "Epoch: 35, Loss: 1.0037, Train: 0.8640 Test: 0.6912\n",
      "Epoch: 36, Loss: 1.7588, Train: 0.9044 Test: 0.6765\n",
      "Epoch: 37, Loss: 1.0567, Train: 0.8750 Test: 0.6324\n",
      "Epoch: 38, Loss: 0.8588, Train: 0.8676 Test: 0.6324\n",
      "Epoch: 39, Loss: 0.9104, Train: 0.9449 Test: 0.7059\n",
      "Epoch: 40, Loss: 0.4715, Train: 0.9154 Test: 0.7059\n",
      "Epoch: 41, Loss: 0.8319, Train: 0.9375 Test: 0.7206\n",
      "Epoch: 42, Loss: 0.4467, Train: 0.9081 Test: 0.6618\n",
      "Epoch: 43, Loss: 0.4561, Train: 0.9118 Test: 0.6618\n",
      "Epoch: 44, Loss: 0.4532, Train: 0.9559 Test: 0.7206\n",
      "Epoch: 45, Loss: 0.2130, Train: 0.9449 Test: 0.7206\n",
      "Epoch: 46, Loss: 0.3315, Train: 0.9890 Test: 0.7206\n",
      "Epoch: 47, Loss: 0.0719, Train: 0.9596 Test: 0.6765\n",
      "Epoch: 48, Loss: 0.1353, Train: 0.9632 Test: 0.6471\n",
      "Epoch: 49, Loss: 0.1350, Train: 0.9926 Test: 0.6912\n",
      "Epoch: 50, Loss: 0.0207, Train: 0.9743 Test: 0.7059\n",
      "tensor([[   0.0000,  -80.6017, -363.1209,  ..., -360.8398, -148.4902,\n",
      "         -318.5233],\n",
      "        [   0.0000,  -85.7036, -286.3676,  ..., -251.3749, -245.3226,\n",
      "         -231.8868],\n",
      "        [   0.0000,  -46.4938, -391.9218,  ..., -395.3779, -202.3903,\n",
      "         -265.1664],\n",
      "        ...,\n",
      "        [   0.0000,  -79.4467, -235.7938,  ..., -362.7146, -179.6377,\n",
      "         -225.0268],\n",
      "        [   0.0000, -127.3158, -313.7322,  ..., -340.5431, -255.6388,\n",
      "         -201.3583],\n",
      "        [   0.0000, -102.7043, -363.6089,  ..., -431.8968, -225.9398,\n",
      "         -262.4576]])\n",
      "torch.Size([23606, 16])\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(untrained_embedding, data.edge_index, data.edge_type)\n",
    "    loss = F.nll_loss(out[data.train_idx], data.train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(untrained_embedding, data.edge_index, data.edge_type).argmax(dim=-1)\n",
    "    train_acc = pred[data.train_idx].eq(data.train_y).to(torch.float).mean()\n",
    "    test_acc = pred[data.test_idx].eq(data.test_y).to(torch.float).mean()\n",
    "    return train_acc.item(), test_acc.item()\n",
    "\n",
    "# Train for 50 epochs to get trained node embedding of size (data.num_nodes, embedding_dim)\n",
    "for epoch in range(1, 51):\n",
    "    loss, embedding = train()\n",
    "    train_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f} '\n",
    "          f'Test: {test_acc:.4f}')\n",
    "    \n",
    "print(model.node_embeddings)\n",
    "print(model.node_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
